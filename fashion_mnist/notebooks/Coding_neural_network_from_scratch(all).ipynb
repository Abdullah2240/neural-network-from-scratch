{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414f8775-01d5-4897-953f-5b0ad72590c5",
   "metadata": {},
   "source": [
    "# Coding_neurons_and_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5218aa16-929c-42e5-a451-fcb9f5adb6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8000000000000003\n"
     ]
    }
   ],
   "source": [
    "# Neuron with 3 inputs\n",
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, -0.4, 0.8]\n",
    "bias = 2\n",
    "result = (inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2a0596-1e72-4c08-8975-5b6adf277146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9600000000000004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Neuron with 4 inputs\n",
    "inputs = [1, 2, 3, 4]\n",
    "weights = [0.2, -0.4, 0.8, -0.71]\n",
    "bias = 3\n",
    "result = (inputs[0]*weights[0] +\n",
    "          inputs[1]*weights[1] +\n",
    "          inputs[2]*weights[2] +\n",
    "          inputs[3]*weights[3] + bias)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6bbc8-12d3-46ec-b6e2-3f385cea51d2",
   "metadata": {},
   "source": [
    "# Cool now do a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fda165d-8a9f-4b25-b59b-c13de818ea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.5, 38.0, 35.5]\n"
     ]
    }
   ],
   "source": [
    "# Layer of 3 neurons each with 5 inputs\n",
    "inputs = [2, 4, 0.5, -3, 5]\n",
    "\n",
    "w = [[1, 2, 3, 4, 5],\n",
    "     [2, 3, 4, 5, 6],\n",
    "    [3, 4, 5, 6, 7]]\n",
    "\n",
    "bias = [2, 5, -6]\n",
    "\n",
    "results = []\n",
    "for i in range(len(w)):\n",
    "    temp = 0\n",
    "    for j in range(len(w[i])):\n",
    "        temp += inputs[j]*w[i][j]\n",
    "    results.append(temp + bias\n",
    "        [i])\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c65c5-1dd7-4a2c-a7a5-0250c4be80c5",
   "metadata": {},
   "source": [
    "# better looping in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673e3140-f384-4fce-98ab-42b761c097ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.5, 38.0, 35.5]\n"
     ]
    }
   ],
   "source": [
    "# Layer of 3 neurons each with 5 inputs but with more verbose and less i using loops\n",
    "inputs = [2, 4, 0.5, -3, 5]\n",
    "\n",
    "weights = [[1, 2, 3, 4, 5],\n",
    "     [2, 3, 4, 5, 6],\n",
    "    [3, 4, 5, 6, 7]]\n",
    "\n",
    "biases = [2, 5, -6]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weight, neuron_bias in zip(weights, biases):\n",
    "    result = 0\n",
    "    for weight, n_input in zip(neuron_weight, inputs):\n",
    "        result += weight*n_input\n",
    "    layer_outputs.append(result + neuron_bias)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5507ae-658a-4aca-b6fc-f62f451f19ea",
   "metadata": {},
   "source": [
    "# Now same thing but in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f31297-9df1-4783-8ef8-fda441987993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db7d1f9-b5cc-48e1-bf3f-1adc99118f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.5]\n"
     ]
    }
   ],
   "source": [
    "# Coding a neuron in numpy\n",
    "inputs = np.array([2, 4, 0.5, -3, 5])\n",
    "weights = np.array([1, 2, 3, 4, 5])\n",
    "bias = 2\n",
    "\n",
    "result = np.dot(weights, inputs.reshape(len(inputs), 1)) + bias\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af45cae7-4f4a-4b73-8af6-85dd532d5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.5 38.  35.5]\n"
     ]
    }
   ],
   "source": [
    "# Coding a layer of neurons in numpy\n",
    "inputs = np.array([2, 4, 0.5, -3, 5])\n",
    "\n",
    "weights = np.array([[1, 2, 3, 4, 5],\n",
    "                    [2, 3, 4, 5, 6],\n",
    "                    [3, 4, 5, 6, 7]])\n",
    "\n",
    "biases = np.array([2, 5, -6])\n",
    "\n",
    "layer_outputs = np.dot(inputs, weights.T) + biases\n",
    "print(layer_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab0d164-322f-4b77-8bb1-9893d70a6dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  26.5   38.    35.5]\n",
      " [ -97.   -78.   -73. ]\n",
      " [-198.  -210.  -236. ]]\n"
     ]
    }
   ],
   "source": [
    "# Coding a batch of inputs being fed to a layer of neurons in numpy\n",
    "inputs = np.array([[2, 4, 0.5, -3, 5],\n",
    "                   [1, 67, 32, -90, 6],\n",
    "                   [34, 65, -97, -12, -5]])\n",
    "\n",
    "weights = np.array([[1, 2, 3, 4, 5],\n",
    "                    [2, 3, 4, 5, 6], \n",
    "                    [3, 4, 5, 6, 7]])\n",
    "\n",
    "biases = np.array([2, 5, -6])\n",
    "\n",
    "layer_outputs = np.dot(inputs, weights.T) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7522af-31d0-42e4-a256-7540c4ebf3e7",
   "metadata": {},
   "source": [
    "# A forward pass thingy without activation (2 hidden layers of neuron with batch inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd9c2ff-f8ad-4ec8-bc4c-f3ab1d49a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  211.   385.   550.]\n",
      " [ -470.  -861. -1276.]\n",
      " [-1324. -2437. -3512.]]\n"
     ]
    }
   ],
   "source": [
    "# Coding a batch of inputs being fed to a layer of neurons in numpy\n",
    "inputs = np.array([[2, 4, 0.5, -3, 5],\n",
    "                   [1, 67, 32, -90, 6],\n",
    "                   [34, 65, -97, -12, -5]])\n",
    "\n",
    "# layer 1\n",
    "weights1 = np.array([[1, 2, 3, 4, 5],\n",
    "                    [2, 3, 4, 5, 6], \n",
    "                    [3, 4, 5, 6, 7]])\n",
    "\n",
    "biases1 = np.array([2, 5, -6])\n",
    "\n",
    "# layer 2\n",
    "weights2 = np.array([[1, 2, 3],\n",
    "                    [2, 3, 6], \n",
    "                    [3, 6, 7]])\n",
    "\n",
    "biases2 = np.array([2, 5, -6])\n",
    "\n",
    "\n",
    "layer_outputs_1 = np.dot(inputs, weights1.T) + biases1\n",
    "layer_outputs_2 = np.dot(layer_outputs_1, weights2.T) + biases2\n",
    "\n",
    "print(layer_outputs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bbb34-42cf-44e1-add4-9b5171c2318c",
   "metadata": {},
   "source": [
    "# Dense Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ad73e6-84bf-4f95-a3dd-483c90724741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28mself\u001b[39m.outputs = np.dot(inputs, \u001b[38;5;28mself\u001b[39m.weights) + \u001b[38;5;28mself\u001b[39m.biases\n\u001b[32m     14\u001b[39m X, y = spiral_data(samples=\u001b[32m200\u001b[39m, classes=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m dense1 = \u001b[43mDenseLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m dense1.forward(X)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(dense1.outputs[:\u001b[32m5\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mDenseLayer.__init__\u001b[39m\u001b[34m(self, n_inputs, n_neurons)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_inputs, n_neurons):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# initializes the weights\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights = \u001b[32m0.01\u001b[39m * \u001b[43mnp\u001b[49m.random.randn(n_inputs, n_neurons)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.biases = np.zeros((\u001b[32m1\u001b[39m, n_neurons))\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import vertical_data\n",
    "nnfs.init()\n",
    "# Dense Layer\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initializes the weights\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "X, y = spiral_data(samples=200, classes=4)\n",
    "\n",
    "dense1 = DenseLayer(2, 3)\n",
    "dense1.forward(X)\n",
    "print(dense1.outputs[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc17f0-9374-43be-9ee3-d83ff8898b3a",
   "metadata": {},
   "source": [
    "# ReLu Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3487375d-e569-4eff-a299-72c25d8c46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU class\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7266e-cc3c-475b-bdf1-d4760aee936a",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a943c5cb-01fa-4f26-bc13-47e6c12cd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_inputs = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_inputs / np.sum(exp_inputs, axis=1, keepdims=True)\n",
    "        self.outputs = probabilities        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94efd36-b7a8-4494-9300-021db4a23233",
   "metadata": {},
   "source": [
    "# Full pass without Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a9a3fab-67f1-4860-988e-285772ec847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333337  0.3333326  0.33333373]\n",
      " [0.33333346 0.33333284 0.33333364]\n",
      " [0.33333457 0.3333309  0.33333457]\n",
      " [0.33333477 0.3333303  0.33333492]]\n"
     ]
    }
   ],
   "source": [
    "# Full pass without Loss\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "inputs = [[1, 7], [13, 4], [54, 3]]\n",
    "\n",
    "dense1 = DenseLayer(2, 3)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = DenseLayer(3, 3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.outputs)\n",
    "dense2.forward(activation1.outputs)\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "print(activation2.outputs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95485eef-acf9-446f-8533-7cc49882719d",
   "metadata": {},
   "source": [
    "# Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147fd1da-393c-4913-b173-334a3ab2e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3efa3d-dcb6-4079-868d-0d4091fcb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum(y_true*y_pred_clipped, axis=1)\n",
    "             \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e97d0-c0c3-4fb6-b8ac-ebb3c26990a6",
   "metadata": {},
   "source": [
    "# One forward Pass with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db4757a-e61e-40d2-8279-4c58707eed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333336  0.33333302 0.3333333 ]\n",
      " [0.3333341  0.3333326  0.33333334]\n",
      " [0.3333355  0.33333147 0.33333305]\n",
      " [0.33333603 0.33333093 0.333333  ]]\n",
      "loss:  1.0986149\n",
      "accuracy:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Full pass without Loss\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "inputs = [[1, 7], [13, 4], [54, 3]]\n",
    "\n",
    "dense1 = DenseLayer(2, 3)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = DenseLayer(3, 3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.outputs)\n",
    "dense2.forward(activation1.outputs)\n",
    "activation2.forward(dense2.outputs)\n",
    "loss = loss_function.calculate(activation2.outputs, y)\n",
    "\n",
    "print(activation2.outputs[:5])\n",
    "\n",
    "print(\"loss: \", loss)\n",
    "\n",
    "predictions = np.argmax(activation2.outputs, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b2d11-4372-40ae-a168-20beb2ec377f",
   "metadata": {},
   "source": [
    "# Optimizing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8295202-0959-4bc0-ba46-f014ffd93f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.0986782 acc:  0.3333333333333333 iter:  1\n",
      "loss:  1.0986092 acc:  0.3333333333333333 iter:  10\n"
     ]
    }
   ],
   "source": [
    "# Random values of weights and biases hoping for better results\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = DenseLayer(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# some helper variables\n",
    "lowest_loss = 9999999\n",
    "iter = 0\n",
    "lowest_dense1_weights = dense1.weights.copy()\n",
    "lowest_dense1_biases = dense1.biases.copy()\n",
    "lowest_dense2_weights = dense2.weights.copy()\n",
    "lowest_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.rand(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.rand(1, 3)\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    loss = loss_function.calculate(activation2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(activation2.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    iter+=1\n",
    "    if loss < lowest_loss:\n",
    "        lowest_dense1_weights = dense1.weights.copy()\n",
    "        lowest_dense1_biases = dense1.biases.copy()\n",
    "        lowest_dense2_weights = dense2.weights.copy()\n",
    "        lowest_dense2_biases = dense2.biases.copy() \n",
    "        print(\"loss: \", loss, 'acc: ', accuracy, \"iter: \", iter)\n",
    "        lowest_loss = loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be127d70-e5ef-49b3-a45d-681f94a93465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "loss:  1.098535 acc:  0.3333333333333333 New set of best values found, iteration:  0\n",
      "loss:  1.0981333 acc:  0.3333333333333333 New set of best values found, iteration:  10\n",
      "loss:  1.0951539 acc:  0.3333333333333333 New set of best values found, iteration:  30\n",
      "loss:  1.0916325 acc:  0.36666666666666664 New set of best values found, iteration:  60\n",
      "loss:  1.090195 acc:  0.3333333333333333 New set of best values found, iteration:  70\n",
      "loss:  1.0854732 acc:  0.3333333333333333 New set of best values found, iteration:  90\n",
      "loss:  1.0821099 acc:  0.3333333333333333 New set of best values found, iteration:  100\n",
      "loss:  1.073277 acc:  0.3333333333333333 New set of best values found, iteration:  120\n",
      "loss:  1.0683508 acc:  0.35 New set of best values found, iteration:  130\n",
      "loss:  1.0588216 acc:  0.38 New set of best values found, iteration:  150\n",
      "loss:  1.0529004 acc:  0.45 New set of best values found, iteration:  170\n",
      "loss:  1.0470719 acc:  0.55 New set of best values found, iteration:  180\n",
      "loss:  1.034522 acc:  0.57 New set of best values found, iteration:  200\n",
      "loss:  1.0276668 acc:  0.6133333333333333 New set of best values found, iteration:  210\n",
      "loss:  1.0233293 acc:  0.6466666666666666 New set of best values found, iteration:  220\n",
      "loss:  1.0169932 acc:  0.6866666666666666 New set of best values found, iteration:  230\n",
      "loss:  1.01082 acc:  0.62 New set of best values found, iteration:  240\n",
      "loss:  1.0048927 acc:  0.7166666666666667 New set of best values found, iteration:  250\n",
      "loss:  0.9940538 acc:  0.63 New set of best values found, iteration:  270\n",
      "loss:  0.9897357 acc:  0.7066666666666667 New set of best values found, iteration:  280\n",
      "loss:  0.98487335 acc:  0.69 New set of best values found, iteration:  290\n",
      "loss:  0.97538227 acc:  0.69 New set of best values found, iteration:  310\n",
      "loss:  0.96367127 acc:  0.66 New set of best values found, iteration:  320\n",
      "loss:  0.94937235 acc:  0.6566666666666666 New set of best values found, iteration:  330\n",
      "loss:  0.91023546 acc:  0.6666666666666666 New set of best values found, iteration:  390\n",
      "loss:  0.8951003 acc:  0.7133333333333334 New set of best values found, iteration:  410\n",
      "loss:  0.886377 acc:  0.72 New set of best values found, iteration:  420\n",
      "loss:  0.86920255 acc:  0.8566666666666667 New set of best values found, iteration:  440\n",
      "loss:  0.85887206 acc:  0.85 New set of best values found, iteration:  450\n",
      "loss:  0.8451592 acc:  0.8533333333333334 New set of best values found, iteration:  470\n",
      "loss:  0.835624 acc:  0.8933333333333333 New set of best values found, iteration:  480\n",
      "loss:  0.78795224 acc:  0.6933333333333334 New set of best values found, iteration:  560\n",
      "loss:  0.7699352 acc:  0.7466666666666667 New set of best values found, iteration:  570\n",
      "loss:  0.76373327 acc:  0.6666666666666666 New set of best values found, iteration:  580\n",
      "loss:  0.7497591 acc:  0.7566666666666667 New set of best values found, iteration:  590\n",
      "loss:  0.7386489 acc:  0.8733333333333333 New set of best values found, iteration:  610\n",
      "loss:  0.6939856 acc:  0.77 New set of best values found, iteration:  670\n",
      "loss:  0.692257 acc:  0.7633333333333333 New set of best values found, iteration:  680\n",
      "loss:  0.67385375 acc:  0.84 New set of best values found, iteration:  690\n",
      "loss:  0.6545374 acc:  0.8366666666666667 New set of best values found, iteration:  710\n",
      "loss:  0.64207286 acc:  0.8333333333333334 New set of best values found, iteration:  730\n",
      "loss:  0.6335878 acc:  0.84 New set of best values found, iteration:  750\n",
      "loss:  0.6276654 acc:  0.91 New set of best values found, iteration:  760\n",
      "loss:  0.58882874 acc:  0.9233333333333333 New set of best values found, iteration:  820\n",
      "loss:  0.5817843 acc:  0.8933333333333333 New set of best values found, iteration:  830\n",
      "loss:  0.5765385 acc:  0.92 New set of best values found, iteration:  840\n",
      "loss:  0.5591882 acc:  0.8933333333333333 New set of best values found, iteration:  870\n",
      "loss:  0.5463905 acc:  0.8733333333333333 New set of best values found, iteration:  900\n",
      "loss:  0.53667504 acc:  0.9233333333333333 New set of best values found, iteration:  920\n",
      "loss:  0.52356476 acc:  0.9533333333333334 New set of best values found, iteration:  940\n",
      "loss:  0.5138796 acc:  0.93 New set of best values found, iteration:  960\n",
      "loss:  0.50110227 acc:  0.95 New set of best values found, iteration:  990\n"
     ]
    }
   ],
   "source": [
    "# Randomly adjusting values of weights and biases\n",
    "import nnfs\n",
    "from nnfs.datasets import vertical_data\n",
    "X, y = vertical_data(samples=100, classes=3)\n",
    "print(2)\n",
    "dense1 = DenseLayer(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# some helper variables\n",
    "lowest_loss = 9999999\n",
    "lowest_dense1_weights = dense1.weights.copy()\n",
    "lowest_dense1_biases = dense1.biases.copy()\n",
    "lowest_dense2_weights = dense2.weights.copy()\n",
    "lowest_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    dense1.weights += 0.01 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.01 * np.random.rand(1, 3)\n",
    "    dense2.weights += 0.01 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.01 * np.random.rand(1, 3)\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    loss = loss_function.calculate(activation2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(activation2.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if loss < lowest_loss:\n",
    "        lowest_dense1_weights = dense1.weights.copy()\n",
    "        lowest_dense1_biases = dense1.biases.copy()\n",
    "        lowest_dense2_weights = dense2.weights.copy()\n",
    "        lowest_dense2_biases = dense2.biases.copy() \n",
    "        if i % 10 == 0:\n",
    "            print(\"loss: \", loss, 'acc: ', accuracy, \"New set of best values found, iteration: \", i)\n",
    "        lowest_loss = loss\n",
    "    else:\n",
    "        dense1.weights = lowest_dense1_weights.copy()\n",
    "        dense1.biases = lowest_dense1_biases.copy()\n",
    "        dense2.weights = lowest_dense2_weights.copy()\n",
    "        dense2.biases = lowest_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "520f8449-5713-46c9-bfb7-3aee369da615",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "    \n",
    "# Loss = ReLU(sum(mul(x0, w0), mul(x1, w1), mul(x2, w2), b0))^2 \n",
    "# Loss = y^2\n",
    "# ReLU = maximum(o, y)\n",
    "\n",
    "\n",
    "# Initialization\n",
    "weights = np.array([1, 2, 3], dtype=\"float64\")\n",
    "bias = 2\n",
    "inputs = np.array([2, -4, 9])\n",
    "learning_rate = 0.001\n",
    "target_output = 0\n",
    "\n",
    "for i in range(100):\n",
    "    # Thangs\n",
    "    linear_output = np.dot(weights, inputs) + bias\n",
    "    output = ReLU(linear_output)\n",
    "    loss = (output - target_output) ** 2\n",
    "\n",
    "    dloss_doutput = 2 * (output - target_output)\n",
    "    doutput_dlinear = ReLU_derivative(linear_output)\n",
    "    dlinear_dweights = inputs\n",
    "    dlinear_dbias = 1.0\n",
    "    \n",
    "    # Final \n",
    "    dloss_dlinear = dloss_doutput * doutput_dlinear\n",
    "    dloss_dweights = dloss_dlinear * dlinear_dweights\n",
    "    dloss_dbiases = dloss_dlinear * dlinear_dbiases\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbiases\n",
    "\n",
    "    print(\"Loss: \", loss)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491aab7-827e-4224-9fbf-4bd3b60ab750",
   "metadata": {},
   "source": [
    "# Backpropogation through a whole Layer (We are geting somewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26e66312-f0d9-44d4-b8b5-85148b9062c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 Loss:  466.56000000000006\n",
      "Iteration:  20 Loss:  5.329595763793193\n",
      "Iteration:  40 Loss:  0.41191524253483786\n",
      "Iteration:  60 Loss:  0.03183621475376345\n",
      "Iteration:  80 Loss:  0.002460565405431671\n",
      "Iteration:  100 Loss:  0.0001901729121621426\n",
      "Iteration:  120 Loss:  1.4698120139337557e-05\n",
      "Iteration:  140 Loss:  1.1359948840900371e-06\n",
      "Iteration:  160 Loss:  8.779778427447647e-08\n",
      "Iteration:  180 Loss:  6.785903626216421e-09\n"
     ]
    }
   ],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "inputs = np.array([1, 2, 3, 4])\n",
    "weights = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "                    [0.5, 0.6, 0.7, 0.8], \n",
    "                    [0.9, 1.0, 1.1 ,1.2]])\n",
    "bias = np.array([0.1, 0.2, 0.3])\n",
    "learning_rate = 0.001\n",
    "target_output = 0.0\n",
    "\n",
    "for i in range(200):\n",
    "    z = np.dot(weights, inputs) + bias\n",
    "    a = ReLU(z)\n",
    "    y = np.sum(a)\n",
    "\n",
    "    # Loss\n",
    "    loss = y ** 2\n",
    "    dL_dy = 2 * y\n",
    "    dy_da = np.ones_like(a)\n",
    "    \n",
    "    dL_da = dL_dy * dy_da\n",
    "    \n",
    "    da_dz = ReLU_derivative(a)\n",
    "\n",
    "    dL_dz = dL_da * da_dz\n",
    "    \n",
    "    dL_dw = np.outer(dL_dz, inputs)\n",
    "    dL_db = dL_dz\n",
    "\n",
    "    # Updating weights and bias\n",
    "    weights -= learning_rate * dL_dw \n",
    "    bias -= learning_rate * dL_db\n",
    "    if i % 20 == 0:\n",
    "        print(\"Iteration: \", i, \"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac82ed-be03-4c10-8790-f8fc4d9d38a8",
   "metadata": {},
   "source": [
    "# Using Matrices in this back prpogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "142dc08a-3ded-4e6f-bf64-33e24b8c78d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# Matrices easen the process\n",
    "\n",
    "\n",
    "# FOR GRADIENT OF LOSS WRT  WEIGHTS\n",
    "# dL_dw = dL_dz Matrix Multiplication dz_dw\n",
    "# dz_dw = inputs\n",
    "# dL_dw = dL_dz Matrix Multiplication inputs\n",
    "# now dL_dz is a matrix like 1 x n and inputs is a matrix of size 1 x n\n",
    "# so we matrix multiply transpose(weights) AND dL_dz\n",
    "# This gives you the dL_dw matrix a single column is the gradients of the weights of a single neuron and the multiple columns are multiple neurons, SO A LAYER OF NEURONS\n",
    "# X_transpose * dL_dz\n",
    "\n",
    "d_values = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "inputs = np.array([[1, 2, 3, 2.5], [2., 5. , -1., 2], [-1.5, 2.7 ,3.3, -0.8]])\n",
    "\n",
    "dweights = np.dot(inputs.T, d_values)\n",
    "\n",
    "print(dweights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2849530-34cf-400f-9b35-44431d67f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6 6]]\n"
     ]
    }
   ],
   "source": [
    "# FOR GRADIENT OF LOSS WRT BIAS\n",
    "# dL_db = dL_dz\n",
    "\n",
    "dvalues = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "\n",
    "biases = np.array([2, 3 ,0.5])\n",
    "\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44842343-bcc0-470e-b46f-d5d768c27440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.41 -0.37]\n",
      " [ 0.88 -0.76 -0.82 -0.74]\n",
      " [ 1.32 -1.14 -1.23 -1.11]]\n"
     ]
    }
   ],
   "source": [
    "# FOR GRADIENT OF LOSS WRT INPUTS\n",
    "# ans = transpose(input) * dz_da\n",
    "# THIS CODE ->>>>>\n",
    "\n",
    "\n",
    "dvalues = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8 ,-0.5, 1], [0.5, -0.91, 0.26 ,-0.5], [-0.26, -0.27, -0.17, -0.87]]).T\n",
    "\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ce66d7-a3d0-474f-ac26-69f2655b9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # initializes the weights\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c738d7-f112-47ab-a1e5-4a98d7781cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU class\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56d39668-9546-4cbd-9644-cb7be52e8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(y_pred)), y_true]\n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum(y_true*y_pred_clipped, axis=1)\n",
    "             \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            # One Hot Encoding\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        # Normalization\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff97f6ad-492a-47fb-af2c-799838ca23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_inputs = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_inputs / np.sum(exp_inputs, axis=1, keepdims=True)\n",
    "        self.outputs = probabilities        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc183a97-83cd-45eb-b80c-6cabcf76de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Classifier - combined with softax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "\n",
    "        return self.loss.calculate(self.outputs, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            # One Hot Encoding\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a0942a7-3b41-4961-8740-2b7430eab40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.0986137\n",
      "Output:  [[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.33333337 0.33333334]]\n",
      "Accuracy:  0.30333333333333334\n",
      "[[ 9.1763563e-05 -1.0102931e-04 -2.3483712e-05]\n",
      " [-1.9180491e-04  5.0279952e-05 -3.1159929e-05]]\n",
      "[[ 2.9173034e-04  1.4010203e-04 -5.6438861e-05]]\n",
      "[[-2.8359846e-05  1.4472018e-05  1.3887829e-05]\n",
      " [ 1.4806271e-04  1.2316163e-04 -2.7122436e-04]\n",
      " [-2.4048159e-05  1.2830078e-05  1.1218081e-05]]\n",
      "[[ 1.8555438e-06 -7.2875991e-08 -1.9639265e-06]]\n",
      "\n",
      "\n",
      "Loss:  1.0986137\n",
      "Output:  [[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.33333337 0.33333334]]\n",
      "Accuracy:  0.30333333333333334\n",
      "[[ 9.1763563e-05 -1.0102931e-04 -2.3483712e-05]\n",
      " [-1.9180491e-04  5.0279952e-05 -3.1159929e-05]]\n",
      "[[ 2.9173034e-04  1.4010203e-04 -5.6438861e-05]]\n",
      "[[-2.8359846e-05  1.4472018e-05  1.3887829e-05]\n",
      " [ 1.4806271e-04  1.2316163e-04 -2.7122436e-04]\n",
      " [-2.4048159e-05  1.2830078e-05  1.1218081e-05]]\n",
      "[[ 1.8555438e-06 -7.2875991e-08 -1.9639265e-06]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(classes=3, samples=100)\n",
    "\n",
    "dense1 = DenseLayer(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(3, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "for i in range(2):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "    \n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Output: \", loss_activation.outputs[:2])\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    print(\"Accuracy: \", acc)\n",
    "\n",
    "    \n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    print(dense1.dweights)    \n",
    "    print(dense1.dbiases)\n",
    "    print(dense2.dweights)\n",
    "    print(dense2.dbiases)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d750949-a32f-4c92-a8a6-6d06540f6cbf",
   "metadata": {},
   "source": [
    "# Optimizers in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d10406-ce3d-4eb3-b4db-c72d7920b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_SGD:\n",
    "    def __init__(self, learning_rate=1):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update_params(self, layer1):\n",
    "        layer1.weights -= self.learning_rate * layer1.dweights\n",
    "        layer1.biases -= self.learning_rate * layer1.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3425a6-d722-4051-9f98-881e6d98caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Optimizers in the current implementation\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "print(2)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Gradient_SGD()\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7580af-af94-45db-b145-3e9ede3fa810",
   "metadata": {},
   "source": [
    "# Decay Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e931e8bc-7c9e-496a-8899-32bec400e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_SDG_wdecay:\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. /( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        layer.weights -= self.current_learning_rate * layer.dweights\n",
    "        layer.biases -= self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d7363d8-396e-4ee8-9442-bd770882e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.300loss: 1.099lr: 1\n",
      "epoch: 1000, acc: 0.413loss: 1.048lr: 0.5002501250625312\n",
      "epoch: 2000, acc: 0.437loss: 1.003lr: 0.33344448149383127\n",
      "epoch: 3000, acc: 0.493loss: 0.952lr: 0.25006251562890724\n",
      "epoch: 4000, acc: 0.513loss: 0.919lr: 0.2000400080016003\n",
      "epoch: 5000, acc: 0.533loss: 0.900lr: 0.16669444907484582\n",
      "epoch: 6000, acc: 0.537loss: 0.879lr: 0.1428775539362766\n",
      "epoch: 7000, acc: 0.553loss: 0.862lr: 0.12501562695336915\n",
      "epoch: 8000, acc: 0.587loss: 0.846lr: 0.11112345816201799\n",
      "epoch: 9000, acc: 0.593loss: 0.832lr: 0.1000100010001\n"
     ]
    }
   ],
   "source": [
    "# Adding a Momentum Gradient Descent Optimizer in the current implementation\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Gradient_SDG_wdecay(1, 1e-3)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f61ae8-5995-4684-b287-c7ecf430a279",
   "metadata": {},
   "source": [
    "# Momentum Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a9314d2-34ec-4475-b3f2-249f3373eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_SDG_Momentum:\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        # current learing rate is the one with alpha\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before updation\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. /( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                            self.current_learning_rate* layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                            self.current_learning_rate* layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    # Call once after updation\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "779b9b54-b954-49c0-844c-3dccc9120aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.373loss: 1.099lr: 1.0\n",
      "epoch: 1000, acc: 0.490loss: 0.938lr: 0.5002501250625312\n",
      "epoch: 2000, acc: 0.507loss: 0.882lr: 0.33344448149383127\n",
      "epoch: 3000, acc: 0.547loss: 0.808lr: 0.25006251562890724\n",
      "epoch: 4000, acc: 0.657loss: 0.746lr: 0.2000400080016003\n",
      "epoch: 5000, acc: 0.663loss: 0.661lr: 0.16669444907484582\n",
      "epoch: 6000, acc: 0.697loss: 0.608lr: 0.1428775539362766\n",
      "epoch: 7000, acc: 0.743loss: 0.568lr: 0.12501562695336915\n",
      "epoch: 8000, acc: 0.760loss: 0.534lr: 0.11112345816201799\n",
      "epoch: 9000, acc: 0.790loss: 0.508lr: 0.1000100010001\n",
      "epoch: 10000, acc: 0.800loss: 0.481lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Trying the whole thing with Momentum Optimizer\n",
    "# Adding a Momentum Gradient Descent Optimizer in the current implementation\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Gradient_SDG_Momentum(decay=1e-3, momentum=0.7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5deed8cb-48ba-4261-ac7c-366e696b26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        # current learing rate is the one with alpha\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Call once before updation\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. /( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        layer.dweights / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        \n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after updation\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45be345f-aec7-4627-8791-8d5d892d7a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333loss: 1.099lr: 0.100\n",
      "epoch: 1000, acc: 0.660loss: 0.761lr: 0.091\n",
      "epoch: 2000, acc: 0.677loss: 0.699lr: 0.083\n",
      "epoch: 3000, acc: 0.683loss: 0.669lr: 0.077\n",
      "epoch: 4000, acc: 0.703loss: 0.651lr: 0.071\n",
      "epoch: 5000, acc: 0.713loss: 0.636lr: 0.067\n",
      "epoch: 6000, acc: 0.730loss: 0.621lr: 0.063\n",
      "epoch: 7000, acc: 0.760loss: 0.565lr: 0.059\n",
      "epoch: 8000, acc: 0.773loss: 0.545lr: 0.056\n",
      "epoch: 9000, acc: 0.770loss: 0.534lr: 0.053\n",
      "epoch: 10000, acc: 0.770loss: 0.524lr: 0.050\n"
     ]
    }
   ],
   "source": [
    "# Trying the whole thing with ADAGRAD Optimizer\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Optimizer_Adagrad(learning_rate=0.1, decay=1e-4)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}' +\n",
    "              f'lr: {optimizer.current_learning_rate:.3f}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b9de3d3-d83d-4861-942f-c8bc1c9787fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSPROP:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        # current learing rate is the one with alpha\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before updation\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. /( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = (self.rho * layer.weight_cache) + ((1-self.rho) * layer.dweights**2)\n",
    "        layer.bias_cache = (self.rho * layer.bias_cache) + ((1 - self.rho) * layer.dbiases**2)\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        layer.dweights / \\\n",
    "                        (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        \n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after updation\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "289788b1-c8e6-414d-ab44-4c7f379657f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.353loss: 1.099lr: 0.02000000\n",
      "epoch: 1000, acc: 0.587loss: 0.852lr: 0.01980218\n",
      "epoch: 2000, acc: 0.623loss: 0.792lr: 0.01960804\n",
      "epoch: 3000, acc: 0.620loss: 0.802lr: 0.01941766\n",
      "epoch: 4000, acc: 0.650loss: 0.731lr: 0.01923095\n",
      "epoch: 5000, acc: 0.657loss: 0.688lr: 0.01904780\n",
      "epoch: 6000, acc: 0.677loss: 0.623lr: 0.01886810\n",
      "epoch: 7000, acc: 0.657loss: 0.649lr: 0.01869176\n",
      "epoch: 8000, acc: 0.710loss: 0.555lr: 0.01851869\n",
      "epoch: 9000, acc: 0.763loss: 0.524lr: 0.01834879\n",
      "epoch: 10000, acc: 0.777loss: 0.491lr: 0.01818198\n"
     ]
    }
   ],
   "source": [
    "# Trying the whole thing with RMSPROP Optimizer\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Optimizer_RMSPROP(learning_rate=0.02, decay=1e-5, rho=0.9999)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}' +\n",
    "              f'lr: {optimizer.current_learning_rate:.8f}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e48b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ba597b-17cb-4f04-8412-328b45b3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        # current learing rate is the one with alpha\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "    # Call once before updation\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. /( 1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.beta2 * layer.weight_cache + (1 - self.beta2) * (layer.dweights ** 2)\n",
    "        layer.bias_cache   = self.beta2 * layer.bias_cache   + (1 - self.beta2) * (layer.dbiases ** 2)\n",
    "        \n",
    "        layer.weight_momentums = (self.beta1 * layer.weight_momentums) + ((1 - self.beta1) * layer.dweights)                                                                         \n",
    "        layer.bias_momentums = (self.beta1 * layer.bias_momentums) + ((1 - self.beta1) * layer.dbiases)\n",
    "\n",
    "        weight_momentums_updated = layer.weight_momentums / ((1 - self.beta1**(self.iterations+1)))\n",
    "        bias_momentums_updated = layer.bias_momentums / ((1 - self.beta1**(self.iterations+1)))\n",
    "\n",
    "        weight_cache_updated = layer.weight_cache / ((1 - self.beta2**(self.iterations+1)))\n",
    "        bias_cache_updated = layer.bias_cache / ((1 - self.beta2**(self.iterations+1)))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        weight_momentums_updated / \\\n",
    "                        (np.sqrt(weight_cache_updated) + self.epsilon)\n",
    "        \n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        bias_momentums_updated / \\\n",
    "                        (np.sqrt(bias_cache_updated) + self.epsilon)\n",
    "\n",
    "    # Call once after updation\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34a186de-954a-4308-84e8-46c4ab459b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333loss: 1.099lr: 0.02000000\n",
      "epoch: 100, acc: 0.513loss: 0.902lr: 0.01998022\n",
      "epoch: 200, acc: 0.610loss: 0.736lr: 0.01996028\n",
      "epoch: 300, acc: 0.637loss: 0.670lr: 0.01994038\n",
      "epoch: 400, acc: 0.753loss: 0.592lr: 0.01992052\n",
      "epoch: 500, acc: 0.773loss: 0.525lr: 0.01990070\n",
      "epoch: 600, acc: 0.793loss: 0.479lr: 0.01988091\n",
      "epoch: 700, acc: 0.800loss: 0.450lr: 0.01986117\n",
      "epoch: 800, acc: 0.847loss: 0.406lr: 0.01984147\n",
      "epoch: 900, acc: 0.850loss: 0.378lr: 0.01982180\n",
      "epoch: 1000, acc: 0.860loss: 0.356lr: 0.01980218\n",
      "epoch: 1100, acc: 0.860loss: 0.338lr: 0.01978259\n",
      "epoch: 1200, acc: 0.857loss: 0.326lr: 0.01976304\n",
      "epoch: 1300, acc: 0.850loss: 0.314lr: 0.01974353\n",
      "epoch: 1400, acc: 0.850loss: 0.306lr: 0.01972406\n",
      "epoch: 1500, acc: 0.857loss: 0.298lr: 0.01970463\n",
      "epoch: 1600, acc: 0.850loss: 0.293lr: 0.01968523\n",
      "epoch: 1700, acc: 0.857loss: 0.287lr: 0.01966588\n",
      "epoch: 1800, acc: 0.867loss: 0.282lr: 0.01964656\n",
      "epoch: 1900, acc: 0.867loss: 0.278lr: 0.01962728\n",
      "epoch: 2000, acc: 0.867loss: 0.274lr: 0.01960804\n",
      "epoch: 2100, acc: 0.863loss: 0.272lr: 0.01958883\n",
      "epoch: 2200, acc: 0.877loss: 0.267lr: 0.01956966\n",
      "epoch: 2300, acc: 0.877loss: 0.263lr: 0.01955053\n",
      "epoch: 2400, acc: 0.883loss: 0.260lr: 0.01953144\n",
      "epoch: 2500, acc: 0.877loss: 0.257lr: 0.01951239\n",
      "epoch: 2600, acc: 0.877loss: 0.257lr: 0.01949337\n",
      "epoch: 2700, acc: 0.887loss: 0.253lr: 0.01947439\n",
      "epoch: 2800, acc: 0.873loss: 0.251lr: 0.01945544\n",
      "epoch: 2900, acc: 0.887loss: 0.248lr: 0.01943653\n",
      "epoch: 3000, acc: 0.880loss: 0.250lr: 0.01941766\n",
      "epoch: 3100, acc: 0.880loss: 0.244lr: 0.01939883\n",
      "epoch: 3200, acc: 0.893loss: 0.242lr: 0.01938003\n",
      "epoch: 3300, acc: 0.880loss: 0.240lr: 0.01936127\n",
      "epoch: 3400, acc: 0.887loss: 0.248lr: 0.01934255\n",
      "epoch: 3500, acc: 0.890loss: 0.246lr: 0.01932386\n",
      "epoch: 3600, acc: 0.890loss: 0.235lr: 0.01930521\n",
      "epoch: 3700, acc: 0.893loss: 0.237lr: 0.01928659\n",
      "epoch: 3800, acc: 0.887loss: 0.235lr: 0.01926801\n",
      "epoch: 3900, acc: 0.890loss: 0.230lr: 0.01924946\n",
      "epoch: 4000, acc: 0.890loss: 0.228lr: 0.01923095\n",
      "epoch: 4100, acc: 0.887loss: 0.231lr: 0.01921248\n",
      "epoch: 4200, acc: 0.887loss: 0.226lr: 0.01919404\n",
      "epoch: 4300, acc: 0.897loss: 0.225lr: 0.01917564\n",
      "epoch: 4400, acc: 0.900loss: 0.225lr: 0.01915727\n",
      "epoch: 4500, acc: 0.897loss: 0.223lr: 0.01913894\n",
      "epoch: 4600, acc: 0.893loss: 0.221lr: 0.01912064\n",
      "epoch: 4700, acc: 0.893loss: 0.221lr: 0.01910238\n",
      "epoch: 4800, acc: 0.897loss: 0.233lr: 0.01908415\n",
      "epoch: 4900, acc: 0.900loss: 0.219lr: 0.01906596\n",
      "epoch: 5000, acc: 0.890loss: 0.218lr: 0.01904780\n",
      "epoch: 5100, acc: 0.890loss: 0.217lr: 0.01902968\n",
      "epoch: 5200, acc: 0.890loss: 0.216lr: 0.01901159\n",
      "epoch: 5300, acc: 0.893loss: 0.215lr: 0.01899353\n",
      "epoch: 5400, acc: 0.887loss: 0.214lr: 0.01897551\n",
      "epoch: 5500, acc: 0.893loss: 0.213lr: 0.01895753\n",
      "epoch: 5600, acc: 0.900loss: 0.217lr: 0.01893957\n",
      "epoch: 5700, acc: 0.897loss: 0.212lr: 0.01892165\n",
      "epoch: 5800, acc: 0.890loss: 0.211lr: 0.01890377\n",
      "epoch: 5900, acc: 0.903loss: 0.211lr: 0.01888592\n",
      "epoch: 6000, acc: 0.893loss: 0.210lr: 0.01886810\n",
      "epoch: 6100, acc: 0.897loss: 0.209lr: 0.01885032\n",
      "epoch: 6200, acc: 0.897loss: 0.208lr: 0.01883257\n",
      "epoch: 6300, acc: 0.897loss: 0.207lr: 0.01881485\n",
      "epoch: 6400, acc: 0.893loss: 0.214lr: 0.01879717\n",
      "epoch: 6500, acc: 0.910loss: 0.207lr: 0.01877952\n",
      "epoch: 6600, acc: 0.903loss: 0.206lr: 0.01876190\n",
      "epoch: 6700, acc: 0.893loss: 0.205lr: 0.01874432\n",
      "epoch: 6800, acc: 0.900loss: 0.204lr: 0.01872677\n",
      "epoch: 6900, acc: 0.893loss: 0.204lr: 0.01870925\n",
      "epoch: 7000, acc: 0.910loss: 0.206lr: 0.01869176\n",
      "epoch: 7100, acc: 0.900loss: 0.207lr: 0.01867431\n",
      "epoch: 7200, acc: 0.910loss: 0.204lr: 0.01865689\n",
      "epoch: 7300, acc: 0.910loss: 0.202lr: 0.01863950\n",
      "epoch: 7400, acc: 0.900loss: 0.201lr: 0.01862215\n",
      "epoch: 7500, acc: 0.897loss: 0.202lr: 0.01860482\n",
      "epoch: 7600, acc: 0.903loss: 0.204lr: 0.01858753\n",
      "epoch: 7700, acc: 0.910loss: 0.201lr: 0.01857027\n",
      "epoch: 7800, acc: 0.903loss: 0.201lr: 0.01855305\n",
      "epoch: 7900, acc: 0.900loss: 0.199lr: 0.01853585\n",
      "epoch: 8000, acc: 0.900loss: 0.200lr: 0.01851869\n",
      "epoch: 8100, acc: 0.907loss: 0.198lr: 0.01850156\n",
      "epoch: 8200, acc: 0.903loss: 0.197lr: 0.01848446\n",
      "epoch: 8300, acc: 0.900loss: 0.203lr: 0.01846739\n",
      "epoch: 8400, acc: 0.903loss: 0.198lr: 0.01845035\n",
      "epoch: 8500, acc: 0.907loss: 0.196lr: 0.01843335\n",
      "epoch: 8600, acc: 0.903loss: 0.199lr: 0.01841638\n",
      "epoch: 8700, acc: 0.903loss: 0.194lr: 0.01839943\n",
      "epoch: 8800, acc: 0.907loss: 0.195lr: 0.01838252\n",
      "epoch: 8900, acc: 0.907loss: 0.194lr: 0.01836564\n",
      "epoch: 9000, acc: 0.903loss: 0.192lr: 0.01834879\n",
      "epoch: 9100, acc: 0.910loss: 0.191lr: 0.01833197\n",
      "epoch: 9200, acc: 0.913loss: 0.191lr: 0.01831519\n",
      "epoch: 9300, acc: 0.900loss: 0.190lr: 0.01829843\n",
      "epoch: 9400, acc: 0.910loss: 0.192lr: 0.01828170\n",
      "epoch: 9500, acc: 0.910loss: 0.191lr: 0.01826501\n",
      "epoch: 9600, acc: 0.903loss: 0.193lr: 0.01824834\n",
      "epoch: 9700, acc: 0.903loss: 0.188lr: 0.01823171\n",
      "epoch: 9800, acc: 0.920loss: 0.188lr: 0.01821510\n",
      "epoch: 9900, acc: 0.910loss: 0.188lr: 0.01819853\n",
      "epoch: 10000, acc: 0.910loss: 0.186lr: 0.01818198\n"
     ]
    }
   ],
   "source": [
    "# Trying the whole thing with Adam Optimizer (The real Thang)\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "dense1 = DenseLayer(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dense2.forward(activation1.outputs)\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f}' + \n",
    "              f'loss: {loss:.3f}' +\n",
    "              f'lr: {optimizer.current_learning_rate:.8f}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c795664-0590-4a60-9fb4-8c3204d1d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularization_l1=0., bias_regularization_l1=0., weight_regularization_l2=0., bias_regularization_l2=0.):\n",
    "        # initializes the weights\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularization_l1 = weight_regularization_l1\n",
    "        self.bias_regularization_l1 = bias_regularization_l1\n",
    "        self.weight_regularization_l2 = weight_regularization_l2\n",
    "        self.bias_regularization_l2 = bias_regularization_l2\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "                    \n",
    "        # Changing weights and biases with respect to the regularization loss\n",
    "        if self.weight_regularization_l1 > 0:\n",
    "            dwL1 = np.ones_like(self.weights)\n",
    "            dwL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularization_l1 * dwL1\n",
    "        \n",
    "        if self.bias_regularization_l1 > 0:\n",
    "            dbL1 = np.ones_like(self.biases)\n",
    "            dbL1[self.biases < 0] = -1\n",
    "            self.dbiases = self.bias_regularization_l1 * dbL1\n",
    "            \n",
    "        if self.weight_regularization_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularization_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularization_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularization_l2 * self.biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd201f3-8e4a-428f-be37-ebbd245d8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # Calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # Calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc6c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.outputs = inputs * self.binary_mask\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a79224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.347 data_loss: 1.099 regularization_loss: 1.099 total_loss: 1.099 lr: 0.05000000\n",
      "epoch: 100, acc: 0.583 data_loss: 0.897 regularization_loss: 0.897 total_loss: 0.897 lr: 0.04975372\n",
      "epoch: 200, acc: 0.637 data_loss: 0.826 regularization_loss: 0.826 total_loss: 0.826 lr: 0.04950740\n",
      "epoch: 300, acc: 0.670 data_loss: 0.785 regularization_loss: 0.785 total_loss: 0.785 lr: 0.04926351\n",
      "epoch: 400, acc: 0.653 data_loss: 0.777 regularization_loss: 0.777 total_loss: 0.777 lr: 0.04902201\n",
      "epoch: 500, acc: 0.667 data_loss: 0.794 regularization_loss: 0.794 total_loss: 0.794 lr: 0.04878287\n",
      "epoch: 600, acc: 0.697 data_loss: 0.772 regularization_loss: 0.772 total_loss: 0.772 lr: 0.04854605\n",
      "epoch: 700, acc: 0.690 data_loss: 0.787 regularization_loss: 0.787 total_loss: 0.787 lr: 0.04831151\n",
      "epoch: 800, acc: 0.690 data_loss: 0.707 regularization_loss: 0.707 total_loss: 0.707 lr: 0.04807923\n",
      "epoch: 900, acc: 0.703 data_loss: 0.703 regularization_loss: 0.703 total_loss: 0.703 lr: 0.04784918\n",
      "epoch: 1000, acc: 0.737 data_loss: 0.698 regularization_loss: 0.698 total_loss: 0.698 lr: 0.04762132\n",
      "epoch: 1100, acc: 0.707 data_loss: 0.751 regularization_loss: 0.751 total_loss: 0.751 lr: 0.04739561\n",
      "epoch: 1200, acc: 0.683 data_loss: 0.732 regularization_loss: 0.732 total_loss: 0.732 lr: 0.04717204\n",
      "epoch: 1300, acc: 0.723 data_loss: 0.697 regularization_loss: 0.697 total_loss: 0.697 lr: 0.04695056\n",
      "epoch: 1400, acc: 0.697 data_loss: 0.701 regularization_loss: 0.701 total_loss: 0.701 lr: 0.04673116\n",
      "epoch: 1500, acc: 0.730 data_loss: 0.645 regularization_loss: 0.645 total_loss: 0.645 lr: 0.04651379\n",
      "epoch: 1600, acc: 0.733 data_loss: 0.707 regularization_loss: 0.707 total_loss: 0.707 lr: 0.04629844\n",
      "epoch: 1700, acc: 0.723 data_loss: 0.701 regularization_loss: 0.701 total_loss: 0.701 lr: 0.04608507\n",
      "epoch: 1800, acc: 0.673 data_loss: 0.795 regularization_loss: 0.795 total_loss: 0.795 lr: 0.04587366\n",
      "epoch: 1900, acc: 0.743 data_loss: 0.629 regularization_loss: 0.629 total_loss: 0.629 lr: 0.04566419\n",
      "epoch: 2000, acc: 0.787 data_loss: 0.678 regularization_loss: 0.678 total_loss: 0.678 lr: 0.04545661\n",
      "epoch: 2100, acc: 0.707 data_loss: 0.788 regularization_loss: 0.788 total_loss: 0.788 lr: 0.04525092\n",
      "epoch: 2200, acc: 0.760 data_loss: 0.667 regularization_loss: 0.667 total_loss: 0.667 lr: 0.04504707\n",
      "epoch: 2300, acc: 0.740 data_loss: 0.701 regularization_loss: 0.701 total_loss: 0.701 lr: 0.04484506\n",
      "epoch: 2400, acc: 0.720 data_loss: 0.683 regularization_loss: 0.683 total_loss: 0.683 lr: 0.04464485\n",
      "epoch: 2500, acc: 0.757 data_loss: 0.702 regularization_loss: 0.702 total_loss: 0.702 lr: 0.04444642\n",
      "epoch: 2600, acc: 0.713 data_loss: 0.671 regularization_loss: 0.671 total_loss: 0.671 lr: 0.04424975\n",
      "epoch: 2700, acc: 0.790 data_loss: 0.688 regularization_loss: 0.688 total_loss: 0.688 lr: 0.04405480\n",
      "epoch: 2800, acc: 0.700 data_loss: 0.679 regularization_loss: 0.679 total_loss: 0.679 lr: 0.04386157\n",
      "epoch: 2900, acc: 0.753 data_loss: 0.674 regularization_loss: 0.674 total_loss: 0.674 lr: 0.04367003\n",
      "epoch: 3000, acc: 0.720 data_loss: 0.674 regularization_loss: 0.674 total_loss: 0.674 lr: 0.04348015\n",
      "epoch: 3100, acc: 0.787 data_loss: 0.633 regularization_loss: 0.633 total_loss: 0.633 lr: 0.04329192\n",
      "epoch: 3200, acc: 0.730 data_loss: 0.715 regularization_loss: 0.715 total_loss: 0.715 lr: 0.04310531\n",
      "epoch: 3300, acc: 0.730 data_loss: 0.734 regularization_loss: 0.734 total_loss: 0.734 lr: 0.04292030\n",
      "epoch: 3400, acc: 0.767 data_loss: 0.705 regularization_loss: 0.705 total_loss: 0.705 lr: 0.04273687\n",
      "epoch: 3500, acc: 0.737 data_loss: 0.661 regularization_loss: 0.661 total_loss: 0.661 lr: 0.04255500\n",
      "epoch: 3600, acc: 0.713 data_loss: 0.676 regularization_loss: 0.676 total_loss: 0.676 lr: 0.04237468\n",
      "epoch: 3700, acc: 0.747 data_loss: 0.614 regularization_loss: 0.614 total_loss: 0.614 lr: 0.04219587\n",
      "epoch: 3800, acc: 0.767 data_loss: 0.674 regularization_loss: 0.674 total_loss: 0.674 lr: 0.04201857\n",
      "epoch: 3900, acc: 0.747 data_loss: 0.672 regularization_loss: 0.672 total_loss: 0.672 lr: 0.04184275\n",
      "epoch: 4000, acc: 0.760 data_loss: 0.679 regularization_loss: 0.679 total_loss: 0.679 lr: 0.04166840\n",
      "epoch: 4100, acc: 0.733 data_loss: 0.727 regularization_loss: 0.727 total_loss: 0.727 lr: 0.04149550\n",
      "epoch: 4200, acc: 0.757 data_loss: 0.628 regularization_loss: 0.628 total_loss: 0.628 lr: 0.04132402\n",
      "epoch: 4300, acc: 0.773 data_loss: 0.571 regularization_loss: 0.571 total_loss: 0.571 lr: 0.04115396\n",
      "epoch: 4400, acc: 0.767 data_loss: 0.654 regularization_loss: 0.654 total_loss: 0.654 lr: 0.04098529\n",
      "epoch: 4500, acc: 0.753 data_loss: 0.690 regularization_loss: 0.690 total_loss: 0.690 lr: 0.04081799\n",
      "epoch: 4600, acc: 0.727 data_loss: 0.637 regularization_loss: 0.637 total_loss: 0.637 lr: 0.04065206\n",
      "epoch: 4700, acc: 0.733 data_loss: 0.714 regularization_loss: 0.714 total_loss: 0.714 lr: 0.04048747\n",
      "epoch: 4800, acc: 0.710 data_loss: 0.736 regularization_loss: 0.736 total_loss: 0.736 lr: 0.04032421\n",
      "epoch: 4900, acc: 0.757 data_loss: 0.652 regularization_loss: 0.652 total_loss: 0.652 lr: 0.04016226\n",
      "epoch: 5000, acc: 0.753 data_loss: 0.639 regularization_loss: 0.639 total_loss: 0.639 lr: 0.04000160\n",
      "epoch: 5100, acc: 0.707 data_loss: 0.678 regularization_loss: 0.678 total_loss: 0.678 lr: 0.03984222\n",
      "epoch: 5200, acc: 0.747 data_loss: 0.669 regularization_loss: 0.669 total_loss: 0.669 lr: 0.03968411\n",
      "epoch: 5300, acc: 0.760 data_loss: 0.685 regularization_loss: 0.685 total_loss: 0.685 lr: 0.03952725\n",
      "epoch: 5400, acc: 0.730 data_loss: 0.659 regularization_loss: 0.659 total_loss: 0.659 lr: 0.03937163\n",
      "epoch: 5500, acc: 0.737 data_loss: 0.739 regularization_loss: 0.739 total_loss: 0.739 lr: 0.03921722\n",
      "epoch: 5600, acc: 0.767 data_loss: 0.611 regularization_loss: 0.611 total_loss: 0.611 lr: 0.03906403\n",
      "epoch: 5700, acc: 0.717 data_loss: 0.729 regularization_loss: 0.729 total_loss: 0.729 lr: 0.03891202\n",
      "epoch: 5800, acc: 0.763 data_loss: 0.605 regularization_loss: 0.605 total_loss: 0.605 lr: 0.03876119\n",
      "epoch: 5900, acc: 0.757 data_loss: 0.587 regularization_loss: 0.587 total_loss: 0.587 lr: 0.03861153\n",
      "epoch: 6000, acc: 0.727 data_loss: 0.652 regularization_loss: 0.652 total_loss: 0.652 lr: 0.03846302\n",
      "epoch: 6100, acc: 0.800 data_loss: 0.589 regularization_loss: 0.589 total_loss: 0.589 lr: 0.03831564\n",
      "epoch: 6200, acc: 0.720 data_loss: 0.638 regularization_loss: 0.638 total_loss: 0.638 lr: 0.03816940\n",
      "epoch: 6300, acc: 0.747 data_loss: 0.646 regularization_loss: 0.646 total_loss: 0.646 lr: 0.03802426\n",
      "epoch: 6400, acc: 0.757 data_loss: 0.661 regularization_loss: 0.661 total_loss: 0.661 lr: 0.03788022\n",
      "epoch: 6500, acc: 0.757 data_loss: 0.618 regularization_loss: 0.618 total_loss: 0.618 lr: 0.03773727\n",
      "epoch: 6600, acc: 0.770 data_loss: 0.652 regularization_loss: 0.652 total_loss: 0.652 lr: 0.03759540\n",
      "epoch: 6700, acc: 0.770 data_loss: 0.588 regularization_loss: 0.588 total_loss: 0.588 lr: 0.03745459\n",
      "epoch: 6800, acc: 0.757 data_loss: 0.593 regularization_loss: 0.593 total_loss: 0.593 lr: 0.03731483\n",
      "epoch: 6900, acc: 0.740 data_loss: 0.678 regularization_loss: 0.678 total_loss: 0.678 lr: 0.03717610\n",
      "epoch: 7000, acc: 0.760 data_loss: 0.623 regularization_loss: 0.623 total_loss: 0.623 lr: 0.03703841\n",
      "epoch: 7100, acc: 0.773 data_loss: 0.704 regularization_loss: 0.704 total_loss: 0.704 lr: 0.03690173\n",
      "epoch: 7200, acc: 0.757 data_loss: 0.629 regularization_loss: 0.629 total_loss: 0.629 lr: 0.03676606\n",
      "epoch: 7300, acc: 0.720 data_loss: 0.707 regularization_loss: 0.707 total_loss: 0.707 lr: 0.03663138\n",
      "epoch: 7400, acc: 0.753 data_loss: 0.659 regularization_loss: 0.659 total_loss: 0.659 lr: 0.03649768\n",
      "epoch: 7500, acc: 0.753 data_loss: 0.631 regularization_loss: 0.631 total_loss: 0.631 lr: 0.03636496\n",
      "epoch: 7600, acc: 0.757 data_loss: 0.655 regularization_loss: 0.655 total_loss: 0.655 lr: 0.03623320\n",
      "epoch: 7700, acc: 0.783 data_loss: 0.602 regularization_loss: 0.602 total_loss: 0.602 lr: 0.03610239\n",
      "epoch: 7800, acc: 0.777 data_loss: 0.608 regularization_loss: 0.608 total_loss: 0.608 lr: 0.03597252\n",
      "epoch: 7900, acc: 0.737 data_loss: 0.679 regularization_loss: 0.679 total_loss: 0.679 lr: 0.03584358\n",
      "epoch: 8000, acc: 0.777 data_loss: 0.576 regularization_loss: 0.576 total_loss: 0.576 lr: 0.03571556\n",
      "epoch: 8100, acc: 0.783 data_loss: 0.606 regularization_loss: 0.606 total_loss: 0.606 lr: 0.03558846\n",
      "epoch: 8200, acc: 0.727 data_loss: 0.718 regularization_loss: 0.718 total_loss: 0.718 lr: 0.03546225\n",
      "epoch: 8300, acc: 0.790 data_loss: 0.571 regularization_loss: 0.571 total_loss: 0.571 lr: 0.03533694\n",
      "epoch: 8400, acc: 0.753 data_loss: 0.629 regularization_loss: 0.629 total_loss: 0.629 lr: 0.03521251\n",
      "epoch: 8500, acc: 0.757 data_loss: 0.680 regularization_loss: 0.680 total_loss: 0.680 lr: 0.03508895\n",
      "epoch: 8600, acc: 0.777 data_loss: 0.586 regularization_loss: 0.586 total_loss: 0.586 lr: 0.03496626\n",
      "epoch: 8700, acc: 0.787 data_loss: 0.615 regularization_loss: 0.615 total_loss: 0.615 lr: 0.03484442\n",
      "epoch: 8800, acc: 0.753 data_loss: 0.602 regularization_loss: 0.602 total_loss: 0.602 lr: 0.03472343\n",
      "epoch: 8900, acc: 0.810 data_loss: 0.539 regularization_loss: 0.539 total_loss: 0.539 lr: 0.03460327\n",
      "epoch: 9000, acc: 0.767 data_loss: 0.752 regularization_loss: 0.752 total_loss: 0.752 lr: 0.03448395\n",
      "epoch: 9100, acc: 0.757 data_loss: 0.607 regularization_loss: 0.607 total_loss: 0.607 lr: 0.03436544\n",
      "epoch: 9200, acc: 0.793 data_loss: 0.611 regularization_loss: 0.611 total_loss: 0.611 lr: 0.03424775\n",
      "epoch: 9300, acc: 0.787 data_loss: 0.633 regularization_loss: 0.633 total_loss: 0.633 lr: 0.03413086\n",
      "epoch: 9400, acc: 0.780 data_loss: 0.635 regularization_loss: 0.635 total_loss: 0.635 lr: 0.03401476\n",
      "epoch: 9500, acc: 0.777 data_loss: 0.621 regularization_loss: 0.621 total_loss: 0.621 lr: 0.03389945\n",
      "epoch: 9600, acc: 0.747 data_loss: 0.638 regularization_loss: 0.638 total_loss: 0.638 lr: 0.03378493\n",
      "epoch: 9700, acc: 0.780 data_loss: 0.609 regularization_loss: 0.609 total_loss: 0.609 lr: 0.03367117\n",
      "epoch: 9800, acc: 0.710 data_loss: 0.656 regularization_loss: 0.656 total_loss: 0.656 lr: 0.03355817\n",
      "epoch: 9900, acc: 0.733 data_loss: 0.643 regularization_loss: 0.643 total_loss: 0.643 lr: 0.03344593\n",
      "epoch: 10000, acc: 0.760 data_loss: 0.650 regularization_loss: 0.650 total_loss: 0.650 lr: 0.03333444\n",
      "evaluation, acc: 0.750, loss: 0.652099\n"
     ]
    }
   ],
   "source": [
    "# Trying the whole thing with L1/L2 regularization and Dropout\n",
    "X, y = spiral_data(classes=3, samples=100)\n",
    "#dense1 = DenseLayer(2, 64)\n",
    "dense1 = DenseLayer(2, 64, weight_regularization_l2=5e-4, bias_regularization_l2=5e-4)\n",
    "activation1 = Activation_ReLU()\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "dense2 = DenseLayer(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    # Forward Pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs)\n",
    "    dense2.forward(dropout1.outputs)\n",
    "\n",
    "    #Loss Calculation\n",
    "    data_loss = loss_activation.forward(dense2.outputs, y)\n",
    "    regularization_loss = (\n",
    "        loss_activation.loss.regularization_loss(dense1) +\n",
    "        loss_activation.loss.regularization_loss(dense2)       \n",
    "    )\n",
    "\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    acc = np.mean(predictions == y)\n",
    "\n",
    "    # Back Pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {acc:.3f} ' + \n",
    "              f'data_loss: {loss:.3f} ' +\n",
    "              f'regularization_loss: {loss:.3f} ' +\n",
    "              f'total_loss: {loss:.3f} ' +\n",
    "              f'lr: {optimizer.current_learning_rate:.8f}')\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "X_test, y_test = spiral_data(classes=3, samples=100)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.outputs)\n",
    "dense2.forward(activation1.outputs)\n",
    "loss = loss_activation.forward(dense2.outputs, y_test)\n",
    "\n",
    "predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(f\"evaluation, acc: {acc:.3f}, loss: {loss:3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629fdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
